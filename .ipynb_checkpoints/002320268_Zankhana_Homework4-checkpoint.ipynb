{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "603f054e-4221-411b-9560-188b5c429b55",
   "metadata": {},
   "source": [
    "## Homework 4\n",
    "### Zankhana Mehta\n",
    "### 002320268\n",
    "### mehta.zan@northeastern.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03f9872-7881-42f4-bb4f-ada75e3ed808",
   "metadata": {},
   "source": [
    "#### __QUESTION 1__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cdefb6-1bf1-489d-894c-44e64ae54781",
   "metadata": {},
   "source": [
    "Self-attention, or scaled dot-product attention, is a mechanism that lets a Transformer determine the relevance of different words in a sentence when focusing on a specific word. It’s similar to a spotlight illuminating various parts of a sentence as each word is processed. Here’s how it works mathematically:\n",
    "\n",
    "1. Query, Key, and Value: For each word, the self-attention mechanism generates three vectors: Query (Q), Key (K), and Value (V), which are learned during training.\n",
    "2. Attention Scores: The model computes attention scores by calculating the dot product between the Query vector of the current word and the Key vectors of all words in the sentence, indicating the level of focus each word should receive.\n",
    "3. Softmax and Scaling: These attention scores are passed through a softmax function, creating a probability distribution. This distribution helps determine the weight of each Value vector, deciding how much each word’s information will impact the current word’s representation.\n",
    "4. Weighted Sum: Finally, the Value vectors are weighted by the attention scores and summed, forming the new representation for the current word.\n",
    "\n",
    "Multi-head attention is a variation where the model uses multiple sets of Query, Key, and Value vectors to capture different aspects of the relationships within the data. This enhances the model’s ability to focus on diverse patterns in the input.\n",
    "\n",
    "Multi-Head Attention\n",
    "\n",
    "The multi-head attention mechanism consists of several steps. For each attention head, the following operations are performed:\n",
    "\n",
    "1. Splitting the input into head-specific queries, keys, and values:\n",
    "\n",
    "   For each head i where 1≤i≤h:\n",
    "   - Query matrix:\n",
    "     $$ Q^i = X \\cdot W_Q^i $$\n",
    "\n",
    "   - Key matrix:\n",
    "     $$ K^i = X \\cdot W_K^i $$\n",
    "\n",
    "   - Value matrix:\n",
    "     $$ V^i = X \\cdot W_V^i $$\n",
    "\n",
    "2. Calculate the attention weights for each head:\n",
    "\n",
    "   For each head i:\n",
    "\n",
    "   - Attention score matrix:\n",
    "     $$ \\text{Attention\\_Score}^i = Q^i \\cdot (K^i)^T $$\n",
    "\n",
    "   - Scaled attention score:\n",
    "     $$ \\text{Scaled\\_Attention\\_Score}^i = \\frac{\\text{Attention\\_Score}^i}{\\sqrt{d_k}} $$\n",
    "\n",
    "     Here, \\( d_k \\) is the dimension of the key vectors, which helps improve numerical stability.\n",
    "\n",
    "   - Attention weights:\n",
    "     $$ \\text{Attention\\_Weights}^i = \\text{softmax}(\\text{Scaled\\_Attention\\_Score}^i) $$\n",
    "\n",
    "3. Compute the context vector for each head:\n",
    "\n",
    "   For each head i:\n",
    "\n",
    "   - Context vector:\n",
    "     $$ \\text{Context\\_Vector}^i = \\text{Attention\\_Weights}^i \\cdot V^i $$\n",
    "\n",
    "4. Concatenate and linearly transform the head outputs:\n",
    "\n",
    "   - Concatenate the context vectors:\n",
    "     $$ \\text{Concat\\_Context\\_Vectors} = [\\text{Context\\_Vector}^1, \\text{Context\\_Vector}^2, ..., \\text{Context\\_Vector}^h] $$\n",
    "\n",
    "   - Final output:\n",
    "     $$ \\text{Output} = \\text{Concat\\_Context\\_Vectors} \\cdot W_O $$\n",
    "\n",
    "     Here, \\( W_O \\) is the linear transformation matrix that produces the final output from the concatenated context vectors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1075d1-d28f-4ee9-bbca-58f254642b46",
   "metadata": {},
   "source": [
    "#### __QUESTION 2__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc31b6c8-db35-44ec-b9a0-c955c2743fec",
   "metadata": {},
   "source": [
    "#### Part 1\n",
    "Mathematical Definition of Cross-Entropy Loss\n",
    "\n",
    "Given an input x∈Rd and a label y∈{1,2,…,k}, let fW(x) denote the output of a linear classifier with parameters W∈R k×d. For a linear classifier, fW(x) produces a score for each class j, which is calculated as:\n",
    "\n",
    "$$\n",
    "f_W(x)_j = W_j \\cdot x\n",
    "$$\n",
    "\n",
    "where Wj represents the j-th row of W. The output scores are converted to probabilities using the softmax function:\n",
    "\n",
    "$$\n",
    "p(y = j | x, W) = \\frac{e^{f_W(x)_j}}{\\sum_{l=1}^k e^{f_W(x)_l}}\n",
    "$$\n",
    "\n",
    "The cross-entropy loss ℓ(f W(x),y) for a single input-label pair (x,y) is given by:\n",
    "\n",
    "$$\n",
    "\\ell(f_W(x), y) = -\\log p(y | x, W) = -\\log \\left( \\frac{e^{f_W(x)_y}}{\\sum_{l=1}^k e^{f_W(x)_l}} \\right)\n",
    "$$\n",
    "\n",
    "#### Part 2\n",
    "Gradient of the Loss with Respect to W\n",
    "\n",
    "To find the gradient of ℓ(f W(x),y) with respect to W, let’s calculate ∇ Wℓ(f W(x),y):\n",
    "\n",
    "The loss function for a single data point can be rewritten as:\n",
    "\n",
    "$$\n",
    "\\ell(f_W(x), y) = -f_W(x)_y + \\log \\left( \\sum_{l=1}^k e^{f_W(x)_l} \\right)\n",
    "$$\n",
    "\n",
    "Step 1: Gradient of the First Term\n",
    "\n",
    "The partial derivative of the first term −f W(x)y with respect to W is:\n",
    "\n",
    "$$\n",
    "\\nabla_W (-f_W(x)_y) = -x \\cdot \\delta_{j=y}\n",
    "$$\n",
    "\n",
    "where δ j=y is 1 if j=y and 0 otherwise.\n",
    "\n",
    "Step 2: Gradient of the Second Term\n",
    "\n",
    "The second term when differentiated with respect to \\( W \\):\n",
    "\n",
    "$$\n",
    "\\nabla_W \\log \\left( \\sum_{l=1}^k e^{f_W(x)_l} \\right) = \\frac{1}{\\sum_{l=1}^k e^{f_W(x)_l}} \\cdot \\sum_{l=1}^k e^{f_W(x)_l} \\cdot \\nabla_W f_W(x)_l\n",
    "$$\n",
    "\n",
    "For each class \\( j \\), we have:\n",
    "\n",
    "$$\n",
    "\\nabla_W f_W(x)_j = x\n",
    "$$\n",
    "\n",
    "Thus, the gradient of the second term with respect to \\( W \\) is:\n",
    "\n",
    "$$\n",
    "\\nabla_W \\log \\left( \\sum_{l=1}^k e^{f_W(x)_l} \\right) = \\sum_{j=1}^k p(y = j | x, W) \\cdot x\n",
    "$$\n",
    "\n",
    "Step 3: Combining Both Terms\n",
    "\n",
    "Putting the two results together, we get:\n",
    "\n",
    "$$\n",
    "\\nabla_W \\ell(f_W(x), y) = -x \\cdot \\delta_{j=y} + \\sum_{j=1}^k p(y = j | x, W) \\cdot x\n",
    "$$\n",
    "\n",
    "Or, equivalently:\n",
    "\n",
    "$$\n",
    "\\nabla_W \\ell(f_W(x), y) = x \\cdot (p(y | x, W) - \\delta_{y=j})\n",
    "$$\n",
    "\n",
    "#### Part 3\n",
    "Gradient Descent Algorithm for Minimizing l\n",
    "\n",
    "To minimize the loss function over a dataset of n samples  {(x 1,y 1),(x 2,y 2),…,(x n,y n)}, we apply gradient descent.\n",
    "\n",
    "The total loss over all samples is:\n",
    "$$\n",
    "L(W) = \\frac{1}{n} \\sum_{i=1}^n \\ell(f_W(x_i), y_i)\n",
    "$$\n",
    "\n",
    "The gradient descent update rule is:\n",
    "\n",
    "$$\n",
    "W \\leftarrow W - \\eta \\cdot \\frac{1}{n} \\sum_{i=1}^n \\nabla_W \\ell(f_W(x_i), y_i)\n",
    "$$\n",
    "\n",
    "where η is the learning rate./\n",
    "\n",
    "#### Part 4\n",
    "Explanation of AdaGrad (Adaptive Gradient Descent)\n",
    "\n",
    "AdaGrad is an adaptive learning rate algorithm designed to improve gradient descent by adjusting the learning rate based on the history of past gradients. Unlike standard gradient descent, which uses a fixed learning rate, AdaGrad assigns smaller learning rates to parameters with larger gradients over time, which can improve convergence, especially for sparse features.\n",
    "\n",
    "The AdaGrad update for a parameter  W involves maintaining a sum of squared gradients G , and updating each parameter W_j as follows:\n",
    "\n",
    "1. Accumulate squared gradients:\n",
    "\n",
    "   $$\n",
    "   G_{t,j} = G_{t-1,j} + (\\nabla_W \\ell_t)^2\n",
    "   $$\n",
    "\n",
    "2. Update \\( W \\) using a decayed learning rate:\n",
    "\n",
    "   $$\n",
    "   W_j \\leftarrow W_j - \\frac{\\eta}{\\sqrt{G_{t,j} + \\epsilon}} \\cdot \\nabla_W \\ell_t\n",
    "   $$\n",
    "\n",
    "Here, ϵ is a small constant added to prevent division by zero, and G_t,j accumulates information about the magnitude of gradients, allowing AdaGrad to adapt the learning rate for each parameter dynamically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a95cb4e-cd46-4712-bcd1-5494606be8c6",
   "metadata": {},
   "source": [
    "#### __QUESTION 3__\n",
    "\n",
    "#### Part 1\n",
    "The 3 types of anguage models are:\n",
    "\n",
    "a. Encoder-only models: Encoder-only models focus on understanding and processing input text to extract meaningful representations. They are particularly effective for tasks that require comprehension of text rather than generation. These models are designed for understanding tasks such as classification, named entity recognition, and question answering. Encoder-only models are best suited for tasks requiring comprehension of input text and do not generate new text.Example, BERT (Bidirectional Encoder Representations from Transformers). BERT is a prime example of an encoder-only model. It uses a transformer encoder to create deep bidirectional representations of text, meaning it looks at the entire context of a word by considering both the words before and after it.\n",
    "\n",
    "b. Decoder-only model: Decoder-only models focus on generating text based on a given prompt. These models are optimized for generating text. They predict the next word in a sequence based on the preceding words, making them ideal for tasks like text generation, completion, and summarization. Example, GPT-3 is a decoder-only model that excels at generating coherent and contextually relevant text. It can perform a wide range of language tasks based on the context provided in the prompt.\n",
    "\n",
    "c. Encoder-decoder model: Encoder-Decoder models use two components: an encoder to process the input text and a decoder to generate the output text. This architecture is well-suited for tasks that require both understanding and generating text. These models combine an encoder and a decoder to both understand and generate text, making them suitable for complex tasks like translation, summarization, and question generation. Example, T5 (Text-to-Text Transfer Transformer)treats every NLP task as a text-to-text problem, allowing the same model to be used for tasks like translation, summarization, and question answering by converting them into a unified text-to-text framework.\n",
    "\n",
    "Difference between encoder-only and decoder-only model is:\n",
    "1. Training Objective:\n",
    "\n",
    "Encoder-only: Uses masked language modeling (MLM), where words are masked, and the model learns to predict them, focusing on bidirectional context for deeper understanding.\n",
    "\n",
    "Decoder-only: Uses an autoregressive objective, predicting the next word based on previous ones, which aligns with sequential text generation.\n",
    "\n",
    "2. Attention Mechanism:\n",
    "\n",
    "Encoder-only: Uses bidirectional attention, allowing each word to consider all others in the sequence, helping with context-rich understanding.\n",
    "\n",
    "Decoder-only: Uses causal (masked) attention, where each word only attends to itself and previous words, ideal for step-by-step generation.\n",
    "\n",
    "3. Applications:\n",
    "\n",
    "Encoder-only: Best suited for tasks like classification, entity recognition, and sentiment analysis, where text comprehension is crucial.\n",
    "\n",
    "Decoder-only: Ideal for text generation tasks, including story writing, completion, and chatbot responses.\n",
    "\n",
    "4. Contextual Representation:\n",
    "\n",
    "Encoder-only: Produces embeddings with full context, capturing complex relationships within text, which can be used in downstream tasks.\n",
    "\n",
    "Decoder-only: Focuses on sequence flow, prioritizing syntactical coherence and the progression of ideas in generated text.\n",
    "\n",
    "5. Bidirectional vs. Unidirectional:\n",
    "\n",
    "Encoder-only: Bidirectional, leveraging words on both sides of each token, improving its handling of ambiguity and polysemy.\n",
    "\n",
    "Decoder-only: Unidirectional (left-to-right), which is essential for generation but limits full contextual understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8a48a5-b46a-4f9e-b299-07ffe40ecd9e",
   "metadata": {},
   "source": [
    "#### Part 2\n",
    "Recurrent Neural Networks (RNNs) were developed to address the limitations of traditional neural networks, such as Feedforward Neural Networks (FNNs), in handling sequential data. FNNs process each input independently, without considering the order or context of other inputs, making them ineffective for tasks like language modeling, machine translation, speech recognition, and time series analysis, where understanding dependencies between inputs is essential.\n",
    "\n",
    "RNNs overcome these limitations by introducing a recurrent connection that allows information to flow from one time-step to the next. This connection enables RNNs to maintain an internal memory by feeding the output of each step back as an input to the next step. This feedback mechanism allows the model to learn temporal dependencies and effectively process sequences of variable length.\n",
    "\n",
    "Example architecture: An LSTM (Long Short-Term Memory) network is a type of Recurrent Neural Network (RNN) designed to remember important information over long sequences, which helps solve the problem of forgetting long-term dependencies in regular RNNs.\n",
    "\n",
    "LSTMs have a special structure with four main parts:\n",
    "\n",
    "Cell State: This is like the memory of the LSTM. It carries important information across different steps in the sequence, allowing the model to \"remember\" things over time.\n",
    "\n",
    "Forget Gate: This gate decides what information to forget from the memory (cell state). It looks at the previous memory and the current input, and decides which pieces should be discarded.\n",
    "\n",
    "Input Gate: This gate decides what new information should be added to the memory. It takes the current input and the previous memory to figure out what to update in the memory.\n",
    "\n",
    "Output Gate: This gate decides what the next output (hidden state) should be. It looks at the memory and figures out what to send out as the output for the current step.\n",
    "\n",
    "Together, these gates help the LSTM keep track of important information and forget irrelevant details, making it much better than regular RNNs at handling long-term dependencies in sequences, such as in language or time series data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e7d5bf-431a-42cd-960d-686017486459",
   "metadata": {},
   "source": [
    "#### Part 3\n",
    "\n",
    "To build a simple feed-forward neural network (FNN) for a sentiment classification task on a customer review dataset, these are the steps I would implement-\n",
    "\n",
    "**1. Data Preprocessing:**\n",
    "\n",
    "Load and Inspect Data: Load the dataset and check for missing values or other issues.\n",
    "\n",
    "Text Cleaning: Clean the reviews by removing stop words, punctuation, special characters, and converting all text to lowercase.\n",
    "\n",
    "Tokenization: Split the text into tokens (words or subwords). This can be done using libraries like NLTK or spaCy.\n",
    "\n",
    "Vectorization: Convert the tokens into numerical representations. Common methods include:\n",
    "\n",
    "a. Bag of Words (BoW): Represents text as a vector of word counts.\n",
    "\n",
    "b. TF-IDF (Term Frequency-Inverse Document Frequency): Weighs word frequencies based on their importance.\n",
    "\n",
    "c. Word Embeddings: Pre-trained embeddings like Word2Vec or GloVe can be used to represent words in dense vectors.\n",
    "\n",
    "**2. Splitting the Dataset:**\n",
    "\n",
    "Split the dataset into training, validation, and test sets. A common split is 80% for training, 10% for validation, and 10% for testing.\n",
    "\n",
    "**3. Building the Feed-Forward Neural Network:**\n",
    "\n",
    "Input Layer: The input layer size corresponds to the number of features (i.e., the number of tokens or the size of your vectorized data).\n",
    "\n",
    "Hidden Layers: You can add one or more fully connected hidden layers. Each hidden layer should have neurons (nodes) with activation functions like ReLU (Rectified Linear Unit) or Sigmoid.\n",
    "\n",
    "Output Layer: The output layer has one neuron for binary sentiment classification (positive/negative). The activation function for binary classification is usually Sigmoid, which outputs a probability between 0 and 1. For multi-class classification (e.g., positive, neutral, negative), you would use softmax instead of sigmoid.\n",
    "\n",
    "Loss Function: For binary classification, use Binary Cross-Entropy loss. For multi-class classification, use Categorical Cross-Entropy loss.\n",
    "\n",
    "Optimizer: Use an optimizer like Adam or SGD to adjust the weights during training.\n",
    "\n",
    "**4. Training the Model:**\n",
    "\n",
    "Model Compilation: Use an optimizer (e.g., Adam), set the loss function, and choose the metric(s) to evaluate model performance (accuracy).\n",
    "Training: Feed the training data to the model, specifying the number of epochs and batch size. Use the validation set to tune hyperparameters and prevent overfitting.\n",
    "Regularization: Consider using Dropout layers or L2 regularization to avoid overfitting.\n",
    "\n",
    "**5. Evaluating the Model:**\n",
    "\n",
    "Once the model is trained, evaluate its performance on the test set using metrics like accuracy, precision, recall, and F1-score. This helps assess how well the model generalizes to unseen data.\n",
    "\n",
    "**6. Model Tuning:**\n",
    "\n",
    "Tune the model by experimenting with different architectures (e.g., the number of hidden layers, the number of neurons per layer), activation functions, and regularization techniques.\n",
    "Use techniques like cross-validation to ensure your model is not overfitting and is robust.\n",
    "\n",
    "**7. Prospective Code:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "35954355-6a86-4b2b-9283-0e2e34954235",
   "metadata": {},
   "source": [
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(reviews)\n",
    "X = tokenizer.texts_to_sequences(reviews)\n",
    "X = pad_sequences(X, maxlen=100)  # Padding sequences to the same length\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, labels, test_size=0.2)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=100, activation='relu'))  # Input layer\n",
    "model.add(Dropout(0.5))  # Regularization to avoid overfitting\n",
    "model.add(Dense(64, activation='relu'))  # Hidden layer\n",
    "model.add(Dense(1, activation='sigmoid'))  # Output layer (binary classification)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_val, y_val))\n",
    "\n",
    "score = model.evaluate(X_val, y_val)\n",
    "print(f'Validation Accuracy: {score[1]*100}%')\n",
    "\n",
    "predictions = model.predict(X_new)  # X_new is the new review data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0bfd63-e916-4620-938d-f0cba7231229",
   "metadata": {},
   "source": [
    "#### Part 4\n",
    "\n",
    "In Machine Learning, entropy measures the level of disorder or uncertainty in a given dataset or system. It is a metric that quantifies the amount of information in a dataset, and it is commonly used to evaluate the quality of a model and its ability to make accurate predictions.\n",
    "\n",
    "A higher entropy value indicates a more heterogeneous dataset with diverse classes, while a lower entropy signifies a more pure and homogeneous subset of data. \n",
    "\n",
    "For a discrete random variable X with possible outcomes {x1,x2,…,xn} and associated probabilities P(x1​),P(x2),…,P(xn), the entropy H(X) is calculated as:\n",
    "\n",
    "$$\n",
    "H(X) = - \\sum_{i=1}^{n} P(x_i) \\log_2 P(x_i)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "P(xi) is the probability of the i-th outcome.log2 is the logarithm base 2, reflecting the amount of information in bits.\n",
    "\n",
    "In the case of a language model, entropy can be used to measure the uncertainty or randomness in predicting the next word (or token) given a sequence of words. It reflects how much unpredictability exists in the language model's predictions.\n",
    "\n",
    "For a language model, the entropy is calculated based on the predicted probability distribution over the possible next words in the sequence. If the model assigns nearly equal probabilities to all possible next words, the entropy will be high (indicating uncertainty). If the model assigns a very high probability to one word and low probabilities to others, the entropy will be low (indicating a strong prediction).\n",
    "To measure entropy for a language model on a sentence, you would compute the entropy at each time step and take the average over the entire sequence. The entropy at a particular time step t, where the model predicts the next word based on a context Ct, would be:\n",
    "\n",
    "$$\n",
    "H(C_t) = - \\sum_{w \\in V} P(w | C_t) \\log_2 P(w | C_t)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "P(xi) is the true probability of outcome xi\n",
    "\n",
    "Q(xi) is the predicted probability of outcome xi\n",
    "\n",
    "In the case of language models, P(w∣C) is the true distribution of the next word w given context C, and Q(w∣C) is the predicted probability of w given C. The cross-entropy between the true distribution and the model's predictions can be used to measure how well the model is performing.\n",
    "\n",
    "For a language model, cross-entropy can be interpreted as how well the model's predicted probabilities match the actual next words (which are typically given as labels in a supervised learning task). The model is trained to minimize cross-entropy, which pushes it to assign higher probabilities to the correct words.\n",
    "\n",
    "For a single sentence, the cross-entropy loss is the average of the individual losses for each word:\n",
    "$$\n",
    "H(P, Q) = - \\frac{1}{T} \\sum_{t=1}^{T} \\log_2 P(w_t | C_t)\n",
    "$$\n",
    "Where:wt is the target word at time t. Ct is the context at time t. T is the total number of words in the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41df36e5-ad9b-473d-8cde-42c8781a6666",
   "metadata": {},
   "source": [
    "#### Part 5\n",
    "\n",
    "An N-gram model is a type of probabilistic language model that predicts the likelihood of a word based on the preceding N−1 words. The model is based on the assumption that the probability of a word depends only on the previous N−1 words, which is known as the Markov assumption. N-gram models are used in various natural language processing (NLP) tasks, such as speech recognition, machine translation, and text generation. They capture local dependencies and provide a simple but effective way to model language based on word sequences.\n",
    "\n",
    "Let's create a bigram model using the input text: \"Zankhana loves Supervised machine learning\".\n",
    "\n",
    "1. Tokenize the Input Text\n",
    "First, we tokenize the text by splitting it into individual words (tokens):\n",
    "\n",
    "Input text: \"Zankhana loves Supervised machine learning\"\n",
    "Tokens: [\"Zankhana\", \"loves\", \"Supervised\", \"machine\", \"learning\"]\n",
    "\n",
    "2. Create Pairs of Consecutive Words (Bigrams)\n",
    "Now, we generate the bigrams from the list of tokens. Each bigram consists of two consecutive words:\n",
    "\n",
    "Bigrams:\n",
    "\n",
    "(\"Zankhana\", \"loves\")\n",
    "(\"loves\", \"Supervised\")\n",
    "(\"Supervised\", \"machine\")\n",
    "(\"machine\", \"learning\")\n",
    "\n",
    "3. Count the Frequency of Each Bigram\n",
    "Next, we count the frequency of occurrence of each bigram in the corpus. Since our input text only contains one sentence with no repetitions of any bigram, each bigram appears once.\n",
    "\n",
    "Bigram Frequencies:\n",
    "\n",
    "(\"Zankhana\", \"loves\"): 1\n",
    "(\"loves\", \"Supervised\"): 1\n",
    "(\"Supervised\", \"machine\"): 1\n",
    "(\"machine\", \"learning\"): 1\n",
    "\n",
    "4. Calculate the Bigram Probabilities\n",
    "Now, let's calculate the probabilities for each bigram. The probability of a bigram P(w2∣w1) is calculated as the ratio of the number of occurrences of the bigram (w1,w2) to the number of occurrences of w1.\n",
    "\n",
    "For this sentence:\n",
    "\n",
    "The word \"Zankhana\" appears once.\n",
    "The word \"loves\" appears once.\n",
    "The word \"Supervised\" appears once.\n",
    "The word \"machine\" appears once.\n",
    "So, the probability of each bigram can be calculated as follows:\n",
    "$$\n",
    "P(w_2 | w_1) = \\frac{\\text{Count}(w_1)}{\\text{Count}(w_1, w_2)}\n",
    "$$\n",
    "\n",
    " \n",
    "For each bigram in our case:\n",
    "$$\n",
    "P(\"loves\" | \"Zankhana\") = \\frac{1}{1} = 1.0\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\"Supervised\" | \"loves\") = \\frac{1}{1} = 1.0\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\"machine\" | \"Supervised\") = \\frac{1}{1} = 1.0\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\"learning\" | \"machine\") = \\frac{1}{1} = 1.0\n",
    "$$\n",
    "\n",
    "5. Store the Bigram Model\n",
    "Finally, the bigram model consists of the following bigrams with their respective probabilities. The bigram model based on my input text \"Zankhana loves Supervised machine learning\" consists of these bigrams, each with a probability of 1.0 (since there are no repeated words and each word follows exactly one other word in the sequence).If there was a larger corpus with more data, these probabilities would be calculated based on the frequency of the bigrams in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c08110-7329-4ee8-8d7b-99beeee4ec63",
   "metadata": {},
   "source": [
    "#### Part 6\n",
    "\n",
    "Fine-tuning allows users to adapt pre-trained LLMs to more specialized tasks. By fine-tuning a model on a small dataset of task-specific data, you can improve its performance on that task while preserving its general language knowledge.\n",
    "\n",
    "1. Full Fine-Tuning:\n",
    "In full fine-tuning, all layers of a pre-trained model are updated during training on the new dataset. This means that each parameter in the model is adjusted to optimize for the new task.\n",
    "\n",
    "2. Feature Extraction (Frozen Feature Layers):\n",
    "Here, only the final few layers of the model are updated, while earlier layers remain “frozen” (their weights are not changed). The frozen layers act as a feature extractor, and only the task-specific layers at the end are fine-tuned.\n",
    "\n",
    "3. Layer-Wise Fine-Tuning (Progressive Unfreezing):\n",
    "In this approach, fine-tuning is done gradually. Initially, only the top layers are fine-tuned, and then layers closer to the input are progressively “unfrozen” and fine-tuned. This approach allows the model to adapt slowly, preventing drastic changes to the pre-trained model’s features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c20713-8396-418a-b506-12932a0e8539",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
